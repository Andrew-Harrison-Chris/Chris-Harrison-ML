{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Following the series: https://www.youtube.com/playlist?list=PLQVvvaa0QuDcjD5BAw2DxE6OF2tius3V3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1- Neuron Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neuron Computation = $\\sum_{i=0}^{n} w_i * x_i + b$ \n",
    "\n",
    "$n$ is the number of inputs coming into the neuron\n",
    "\n",
    "$w_i$ is the ith weight associated with the ith input\n",
    "\n",
    "$x_i$ is the ith input into the neuron\n",
    "\n",
    "$b$ is a bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.59"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Raw Python: example of simple neuron computation\n",
    "inputs = [1.2,5.3,2.1]\n",
    "weights = [3.1,2.1,9.4]\n",
    "bias = 3\n",
    "\n",
    "output = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + bias\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Coding a Layer\n",
    "\n",
    "\n",
    "<img src=\"img/net1.png\" style=\"width:200px;height:200px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.8, 1.21, 2.385]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Raw Python: Coding a layer with 3 neurons and 4 inputs\n",
    "## We will use numpy later to vectorize all of the computation don't worry (sorry if this is cringy rn)\n",
    "inputs = [1,2,3,2.5]\n",
    "\n",
    "weights1 = [0.2,0.8,-0.5,1]\n",
    "weights2 = [0.5,-0.91,0.26,-0.5]\n",
    "weights3 = [-0.26,-0.27,.17,.87]\n",
    "\n",
    "bias1 = 2\n",
    "bias2 = 3\n",
    "bias3 = 0.5\n",
    "\n",
    "# Notice that output is a 3-dimensional vector (list) since we are coding a layer with 3 neurons\n",
    "outputs = [0,0,0]\n",
    "\n",
    "outputs[0] = inputs[0]*weights1[0] + inputs[1]*weights1[1] + inputs[2]*weights1[2] + inputs[3]*weights1[3] + bias1\n",
    "outputs[1] = inputs[0]*weights2[0] + inputs[1]*weights2[1] + inputs[2]*weights2[2] + inputs[3]*weights2[3] + bias2\n",
    "outputs[2] = inputs[0]*weights3[0] + inputs[1]*weights3[1] + inputs[2]*weights3[2] + inputs[3]*weights3[3] + bias3\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - The Dot Product\n",
    "\n",
    "### Some dimensionality\n",
    "Array: $l = [1,5,6,2]$, Shape: $(4,)$, Type: 1D array/vector\n",
    "\n",
    "Array: $lol = [[1,5,6,2],[3,2,1,3]]$, Shape: $(2,4)$, Type: 2D array/Matrix\n",
    "\n",
    "Array: $lolol = [[[1,5,6,2],[3,2,1,3]],[[5,2,1,2],[6,4,8,4]],[[2,8,5,3],[1,1,9,4]]] $, Shape: $(3,2,4)$, Type: 3D Array/3-Tensor\n",
    "\n",
    "### Definition\n",
    "Let $u = [u_1,u_2,...,u_n]$ and $v = [v_1,v_2,...,v_n]$ be vectors in $R^n$. The dot product between $u$ and $v$ is defined as $u*v = \\sum_{i=1}^{n} u_i*v_i$\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 2.385]\n"
     ]
    }
   ],
   "source": [
    "# Inputs\n",
    "inputs = [1,2,3,2.5]\n",
    "\n",
    "# Weight matrix\n",
    "weights = [[0.2,0.8,-0.5,1],\n",
    "           [0.5,-0.91,0.26,-0.5],\n",
    "           [-0.26,-0.27,.17,.87]]\n",
    "\n",
    "# Vector of biases\n",
    "biases = [2,3,0.5]\n",
    "\n",
    "layer_outputs = [] # Output of current layer\n",
    "\n",
    "# Iterate through each row of the weight matrix and each entry in the bias vector\n",
    "for neuron_weights, neuron_bias in zip(weights,biases):\n",
    "    neuron_output = 0 # Output of the neuron\n",
    "    # Iterate through the inputs and the weights in the current row\n",
    "    for n_input, weight in zip(inputs, neuron_weights):\n",
    "        neuron_output += n_input*weight # Multiply input by associated weight\n",
    "    neuron_output += neuron_bias # Add bias\n",
    "    layer_outputs.append(neuron_output)\n",
    "print(layer_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.8  , 1.21 , 2.385])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numpy implementation\n",
    "# Inputs\n",
    "inputs = [1,2,3,2.5]\n",
    "\n",
    "# Weight matrix\n",
    "weights = [[0.2,0.8,-0.5,1],\n",
    "           [0.5,-0.91,0.26,-0.5],\n",
    "           [-0.26,-0.27,.17,.87]]\n",
    "\n",
    "# Vector of biases\n",
    "biases = [2,3,0.5]\n",
    "\n",
    "output = np.dot(weights,inputs) + biases\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 - Batches, Layers and Objects\n",
    "\n",
    "- Batches allow us to train networks in a parallel fashion. We can show a network multiple samples at a time which can help it to genrealize better. Imagine trying to fit a line to one data point at a time. In this case, the fit line would fluctuate dramatically with each each new data point. If we show all the samples at once, however, this can cause the network to overfit to the training data. A batch size of 32 is pretty common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5031 , -1.04185, -2.03875],\n",
       "       [ 0.2406 , -2.283  , -4.9879 ],\n",
       "       [-0.99314,  1.41254, -0.35655]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numpy implementation\n",
    "# Inputs\n",
    "inputs = [[1,2,3,2.5],\n",
    "          [2.0,5.0,1.0,2.0],\n",
    "          [-1.5,2.7,3.3,-0.8]] # Each row in inputs is an individual training example\n",
    "\n",
    "# Weight matrix\n",
    "# Layer 1\n",
    "weights = [[0.2,0.8,-0.5,1],\n",
    "           [0.5,-0.91,0.26,-0.5],\n",
    "           [-0.26,-0.27,.17,.87]]\n",
    "# Layer2\n",
    "weights2 = [[0.1,-0.14,0.5],\n",
    "           [-0.5,0.12,-0.33],\n",
    "           [-0.44,0.73,-0.13]]\n",
    "\n",
    "# Vector of biases\n",
    "biases = [2,3,0.5]\n",
    "biases2 = [-1,2,-0.5]\n",
    "\n",
    "layer1_outputs = np.dot(inputs,np.array(weights).T) + biases\n",
    "layer2_outputs = np.dot(layer1_outputs,np.array(weights2).T) + biases2\n",
    "layer2_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General feedforward computation in matrix form (without activation function)\n",
    "#### For first layer:\n",
    "$O_1 = XW^T + b$\n",
    "\n",
    "$X$ - batch_size x num_features matrix\n",
    "\n",
    "$W$ - num_output_neurons x num_features matrix (Hence $W^T$ is num_features x num_output_neurons matrix\n",
    "\n",
    "$b$ - num_output_neurons x 1 vector\n",
    "\n",
    "$O_1$ - batch_size x num_output_neurons matrix\n",
    "\n",
    "What is the intuition? Each row of $X$ is an individual observation from the training data that contains the values of each feature for that observation. For instance, each feature of a row could represent that gray-scale value of an individual pixel of an image. When we peform the matrix multiplication $XW^T + b$, for each row in the matrix $X$, we are multilying the value of each feature to its corresponding weight in the respective column of $W^T$, summing these values, and adding a bias term. This results in the output matrix $O_1$ which can we defined as follows:\n",
    "\n",
    "$O_{ij} = $ the output of neuron $j$ in the $i^{th}$ batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.148296  , -0.08397602],\n",
       "       [ 0.20705646, -0.04265608],\n",
       "       [ 0.20124979, -0.07290616]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Layer object\n",
    "np.random.seed(0)\n",
    "\n",
    "# Training data X: a batch_size x n_features matrix\n",
    "X = [[1,2,3,2.5],\n",
    "    [2.0,5.0,1.0,2.0],\n",
    "    [-1.5,2.7,3.3,-0.8]]\n",
    "\n",
    "\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self,n_inputs,n_neurons):\n",
    "        self.weights = 0.1*np.random.randn(n_inputs,n_neurons)\n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "    def forward(self,inputs):\n",
    "        self.output = np.dot(inputs,self.weights) + self.biases\n",
    "        \n",
    "layer1 = Layer_Dense(n_inputs=4,n_neurons=5)\n",
    "layer2 = Layer_Dense(n_inputs=5,n_neurons=2)\n",
    "\n",
    "layer1.forward(X)\n",
    "layer2.forward(layer1.output)\n",
    "layer2.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
